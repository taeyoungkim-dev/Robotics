# 환경: 4x4 크기의 얼어붙은 호수 그리드입니다.
# 목표: 'S'(시작) 지점에서 출발해 'G'(목표, 보상 +1) 지점까지 가는 것입니다.
# 장애물: 중간에 'H'(구멍, 보상 0 및 게임 종료)가 있습니다. 'F'(얼음)는 안전합니다.
# 핵심: 이 호수는 "미끄럽습니다(slippery)". 
# 즉, 에이전트(나)가 '오른쪽'으로 가라고 명령해도, 
# 일정 확률로 위나 아래, 또는 심지어 왼쪽으로 미끄러질 수 있습니다.
강화학습의 'Hello, World'는 단연 'Frozen Lake (얼어붙은 호수)' 문제입니다. 🧊

이것은 코딩에서 print("Hello, World!")가 "출력"이라는 가장 기본적인 기능을 배우게 하는 것처럼, 강화학습의 가장 핵심적인 요소들을 체험하게 해주는 고전적인 문제입니다.

🧊 Frozen Lake: 강화학습의 "Hello, World"
Frozen Lake는 OpenAI의 Gymnasium (이전 Gym) 라이브러리에 포함된 아주 간단한 환경입니다.

환경: 4x4 크기의 얼어붙은 호수 그리드입니다.

목표: 'S'(시작) 지점에서 출발해 'G'(목표, 보상 +1) 지점까지 가는 것입니다.

장애물: 중간에 'H'(구멍, 보상 0 및 게임 종료)가 있습니다. 'F'(얼음)는 안전합니다.

핵심: 이 호수는 "미끄럽습니다(slippery)". 즉, 에이전트(나)가 '오른쪽'으로 가라고 명령해도, 일정 확률로 위나 아래, 또는 심지어 왼쪽으로 미끄러질 수 있습니다.

🤖 이 프로젝트로 무엇을 경험하나요?
이 프로젝트는 복잡한 인공 신경망(딥러닝) 없이, 강화학습의 가장 근본적인 개념들을 직접 코드로 구현하고 눈으로 확인할 수 있게 해줍니다.

핵심 3요소 (상태, 행동, 보상):

상태(State): 내가 현재 16개(4x4)의 칸 중 어디에 있는가?

행동(Action): 상, 하, 좌, 우 중 어느 방향으로 움직일 것인가?

보상(Reward): 목표 지점에 도달했는가(+1) 아니면 구멍에 빠졌는가(0)?

Q-테이블 (Q-Table) 만들기:

이것이 이 프로젝트의 핵심입니다. Q-테이블은 에이전트의 "컨닝 페이퍼"나 "뇌"와 같습니다.

[상태, 행동] 쌍의 모든 조합에 대한 '기대 가치'를 저장하는 간단한 2차원 배열(테이블)입니다. (예: 16개 상태 x 4개 행동 = 16x4 크기의 테이블)

Q[2, 3]의 값이 0.8이라면, "2번 칸(상태)에서 3번 행동(예: 오른쪽)을 하는 것의 기대 가치는 0.8이다"라는 뜻입니다.

탐험(Exploration) vs. 활용(Exploitation):

에이전트는 학습 초기에 무작위로 움직이며(탐험) 어떤 행동이 좋은 보상을 주는지 데이터를 수집합니다.

학습이 진행될수록, 이미 알고 있는 가장 가치 높은 행동(Q-테이블에서 가장 높은 값)을 선택합니다(활용).

학습 과정 (Q-Learning 알고리즘):

에이전트가 한 번 움직이고 보상을 받을 때마다, 이 Q-테이블의 값을 "업데이트"합니다. 이 업데이트 규칙이 바로 Q-Learning이라는 알고리즘입니다.

📝 프로젝트 수행 단계
환경 설치: Python의 gymnasium 라이브러리를 설치합니다.

Bash

pip install gymnasium
환경 로드: Frozen Lake 환경을 불러옵니다.

Python

import gymnasium as gym
# 'is_slippery=True'가 기본값이며, 이것이 이 문제를 흥미롭게 만듭니다.
env = gym.make("FrozenLake-v1", is_slippery=True, render_mode="human")
Q-테이블 초기화: 모든 값을 0으로 채운 2D 배열(리스트)을 만듭니다.

Python

import numpy as np
# 16개의 상태 (4x4)와 4개의 행동 (상하좌우)
q_table = np.zeros([env.observation_space.n, env.action_space.n])
학습 루프 구현:

수천 번의 에피소드(게임 한 판)를 반복합니다.

각 에피소드에서:

(탐험) 가끔 무작위 행동을 선택하거나, (활용) Q-테이블에서 현재 상태의 가장 높은 Q값을 가진 행동을 선택합니다.

env.step(action)을 실행하여 환경에서 한 발짝 움직입니다.

환경으로부터 new_state (새로운 위치), reward (보상), done (게임 종료 여부)를 받습니다.

Q-Learning의 핵심 공식을 사용해 q_table[state, action] 값을 업데이트합니다.

결과 확인: 학습이 끝난 Q-테이블을 기반으로 에이전트가 구멍을 피해 목표 지점까지 완벽하게 길을 찾아가는 것을 env.render()를 통해 시각적으로 확인합니다.

💡 핵심 로직: Q-Learning
이 프로젝트의 "Hello, World!" 한 줄은 바로 Q-Learning 업데이트 공식입니다.

"현재 칸(S)에서 이 행동(A)을 한 가치"는 "방금 받은 보상(R)"과 "다음 칸(S')에서 할 수 있는 최선의 행동의 가치"를 합쳐서 갱신합니다.

이를 간단한 코드로 표현하면 이렇습니다.

Python

# (state, action, reward, new_state는 이미 얻었다고 가정)

# Q-Learning 하이퍼파라미터
learning_rate = 0.1  # 학습률 (알파)
discount_factor = 0.99 # 미래 보상 할인율 (감마)

# Q-테이블 업데이트 (핵심 공식)
old_value = q_table[state, action]
next_max = np.max(q_table[new_state]) # 다음 상태에서 얻을 수 있는 최대 가치

# Q(S, A) = Q(S, A) + α * (R + γ * max(Q(S', a)) - Q(S, A))
new_value = old_value + learning_rate * (reward + discount_factor * next_max - old_value)
q_table[state, action] = new_value
이 간단한 테이블(Q-Table)을 채워나가는 과정 자체가 강화학습 에이전트가 '학습'하는 가장 원시적이고 핵심적인 방법입니다.